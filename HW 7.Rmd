---
title: "HW 7"
author: "Brandon Fenton and Allison Theobold"
date: "Due: October 27, 2016"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
    
header-includes: \usepackage{float} \usepackage{bm} \usepackage{amsmath} \usepackage{amssymb} \usepackage{microtype}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(echo = FALSE)

library(grid)
library(gridExtra)
library(pander)
library(dplyr)
library(ggplot2)
library(effects)
library(ggfortify)
library(parallel)
library(mgcv)
library(nlme)
library(mosaic)

panderOptions('missing', "-")

pander_lm <-function (fit, ...)
{
  fit.sum <- summary(fit)
  fit.coef <- fit.sum$coefficients
  fit.ttable <- matrix(nrow=length(fit.sum$aliased), ncol=4)
  colnames(fit.ttable) <- colnames(fit.sum$coefficients)
  rownames(fit.ttable) <- names(fit.sum$aliased)

  notna <- as.vector(which(!fit.sum$aliased))
  fit.ttable[notna,] <- fit.coef
  fit.ttable <- as.data.frame(fit.ttable)
  fit.ttable$`Pr(>|t|)` <- ifelse(fit.ttable$`Pr(>|t|)` < 0.0001, "<0.0001",
                                     sprintf("%.4f", fit.ttable$`Pr(>|t|)`))
  

  pander(fit.ttable, ...)
}

pander_anova <-function (fit, ...)
{
  fit.anova <- anova(fit)
  fit.anova$`Pr(>F)` <- ifelse(fit.anova$`Pr(>F)` < 0.0001, "<0.0001",
                                  sprintf("%.4f", fit.anova$`Pr(>F)`))

pander(fit.anova, ...)
}

clust <- makeCluster(detectCores())

# Set seeds for cores (runs will always be the same).
clusterSetRNGStream(clust, 1800)

# Set seed for regular random number generation (for the non-parallel stuff)
set.seed(1800)
```


Revisit your CO2 concentration time series from HW 4. If you want to switch groups from that assignment you can or you can work in the same ones. If you switch groups, pick a time series to analyze from those you worked with. Groups of up to 3. And just one time series per group.

1) We are going to be exploring a more complete model that includes a long-term trend and a seasonal component. We need to pick the type of trend and the form of the seasonal component. For the long term trend, consider the following options: no trend, linear trend, or quadratic trend. For the seasonal component, consider no seasonal component, seasonal means, single harmonic pair (m = 1), and 5th order harmonic (m = 5). Consider all combinations of these components, fit using `lm`.  Create a table that contains the model description, model used DF (so the count of free parameters), AICs, and $\Delta$AICs, sorting the table by AIcs. Use this information to discuss the top model selected (what was in it), the strength of support for that model versus the others, and the strength of evidence for a long-term trend and seasonal component (of the type selected) versus not including them in the model. 

  - You should be creating a table that contains twelve models.
```{r p1}
library(TSA)

BRW_flask<-tbl_df(read.table("BRW_flask.txt", header=T)) 

BRW_flask <- filter(BRW_flask, year>=1972)

BRW_flask <- BRW_flask %>% mutate(month = as.factor(month)) %>%
  mutate(year = as.vector(time(ts(BRW_flask$value,start=c(1972,1),freq=12))))

BRW_flask <- ts(BRW_flask, start = c(1972, 1), freq = 12)

# Models 1-3

ym_lm1 <- lm(value ~ 1, data = BRW_flask)
ym_lm2 <- lm(value ~ year, data = BRW_flask)
ym_lm3 <- lm(value ~ poly(year, 2), data = BRW_flask)

# Models 4-6
# no seasonal component is only a trend?
ym_lm4 <- lm(value ~ as.factor(month), data = BRW_flask) #seasonal means
ym_lm5 <- lm(value ~ harmonic(BRW_flask, m = 1), data = BRW_flask) 
ym_lm6 <- lm(value ~ harmonic(BRW_flask, m = 5), data = BRW_flask) 

## Trend with seasonality
ym_lm7 <- lm(value ~ year + as.factor(month), data = BRW_flask)
ym_lm8 <- lm(value ~ year + harmonic(BRW_flask, m = 1), data = BRW_flask)
ym_lm9 <- lm(value ~ year + harmonic(BRW_flask, m = 5), data = BRW_flask)

## Quadratic trend with seasonality
ym_lm10 <- lm(value ~ poly(year, 2) + as.factor(month), data = BRW_flask)
ym_lm11 <- lm(value ~ poly(year, 2) + harmonic(BRW_flask, m = 1), 
             data = BRW_flask)
ym_lm12 <- lm(value ~ poly(year, 2) + harmonic(BRW_flask, m = 5), 
             data = BRW_flask)

DF <- AIC <- rep(NA, 12)
for(i in 1:12){
  DF[i] <- 529 - summary(eval(as.name(paste("ym_lm", i, sep = ""))))$df[2]
  AIC[i] <- AIC(eval(as.name(paste("ym_lm", i, sep = ""))))
}

delta_AIC <- AIC - min(AIC)

table <- cbind(t(t(DF)), t(t(AIC)), t(t(delta_AIC)))
colnames(table) <- c("DF", "AIC", paste(expression(delta), "AIC"))
row.names(table) <- c("Int Only", "Year Trend", "Quadratic Year Trend", 
                      "Seasonal Means", "Single Harmonic", "5th Order Harmonic", 
                      "Year w/ Seasonal Means", "Year w/ Single Harmonic", 
                      "Year w/ 5th Order Harmonic", 
                      "Quadratic w/ Seasonal Means", 
                      "Quadratic w/ Single Harmonic", 
                      "Quadratic w/ 5th Order Harmonic")



table[order(table[, 2]), ]
```

In the table above, describing the fits of the 12 models considered, we see that the model including a quadratic for time, as well as a $5^{th}$ order harmonic fits "the best" according to AIC, however the AIC for the quadratic model with seasonal means is a difference of less than 2 from this "top model". There is strong evidence of the need for a quadratic trend component, as the quadratic models substantially outperform the linear trend models, according to AIC units. As for the seasonal component, there is no evidence of a difference between a model including seasonal means verses a $5^{th}$ order harmonic (after accounting for the quadratic trend). The model including a $5^{th}$ order harmonic does "fit better", but the interpretation of this model is far more challenging than a model which incorporates seasonal means. Therefore, in the interest of parsimony, from this model suite, the quadratic trend model with seasonal means should be chosen.   

2) Now fit a `gam` from the `mgcv` package that includes a long-term trend based on a thin-plate spline with shrinkage that uses `k = #years, bs = "ts"` from the fractional year variable and a cyclic spline seasonal component. To build the cyclic spline component, use the numerically coded month variable that goes from 1 to 12 and `k = 12,bs = "cc"`. Fit the model, plot the long-term trend and the seasonal component (use `plot(gam_model)`), and discuss the estimated components, using both the plots and the EDF of each term.

```{r p2}

ym_gam <- gam(value ~ s(time(BRW_flask), k = 43, bs = "ts")
                    + s(cycle(BRW_flask), k = 12, bs = "cc"), data = BRW_flask)

par(mfrow=c(1,2))
plot(ym_gam)
par(mfrow=c(1,1))
```

For the `gam` fit above, the long-term trend, $\displaystyle{\widehat{s(time)}_{37.462}},$ estimated using a thin-plate spline with shrinkage, shows a continual increasing trend from 1974 to 2015. The spline allows for "rough-ness" to the increasing trend, allowing for dips such as the one near 1995. The cyclic spline seasonal component, $\displaystyle{\widehat{s(season)}_{9.364}},$ estimated using a circular cubic spline, shows little deviations in the first 6 seasons, with a sharp decrease in seasons 7 and 8 (July and August), with an increase for the final seasons.     

3) Calculate the AIC of the GAM using the `AIC` function and discuss how that result compares to your AICs in #1. How is it similar or different in terms of information (degrees of freedom) used?

The AIC for the GAM model is `r floor(AIC(ym_gam))`, which is lower than that for the quadratic model with a fifth-order harmonic by `r floor(AIC(ym_lm12) - AIC(ym_gam))`. In the quadratic model with a fifth-order harmonic, the model used 13 degrees of freedom, 1 for intercept, 2 for the quadratic, and 10 for the harmonics, while the `gam` model used 47.826 degrees of freedom, 1 for intercept, 37.462 for the long-term trend, and 9.364 for the seasonal component. Indeed, the spline model fits these data "better" based on AIC units, however this fit comes with more than tripling the model degrees of freedom compared to the quadratic fifth-order harmonic model. 

TODO not sure about df for GAM. looking it up
OKAY!


4) Compare the fitted values of your GAM to those from your top model, plotting the two models' results and the responses vs time on the same plot.

```{r p4}
plot(BRW_flask[,4])
lines(fitted(ym_lm12) ~ as.vector(time(BRW_flask)), col="red")
lines(fitted(ym_gam) ~ as.vector(time(BRW_flask)), col="blue")
```

## A simulation study with autocorrelation present

5) Revisit your simulation with an AR(1) from HW 6 \# 10. Consider fitting a model with autocorrelation in it using `gls` from the `nlme` package that accounts for an MA(1) error and another that accounts for an AR(1) error. Run your simulation code, extracting the p-values from the two model summaries and estimate the type I error rate in each situation and compare it to what you get from the regular linear model. 

```{r p5, eval=F}

Bozeman <- read.csv("Bozeman.csv", header = T)

monthsF <- sort(unique(Bozeman$MonthRE))

countfun <- function(x) c(sum(x < 32), sum(!is.na(x)))

monthcountMINF <- aggregate(Bozeman$TMIN..F., 
                            by = list(Bozeman$MonthRE), FUN = countfun)
  
yearcountMINF <- aggregate(Bozeman$TMIN..F.,
                           by = list(Bozeman$Year), FUN = countfun)
  
Data1 <- data.frame(Year = yearcountMINF[,1],
                      DaysBelow32 = yearcountMINF$x[,1],
                      MeasuredDays = yearcountMINF$x[,2],
                      PropDays = yearcountMINF$x[,1]/yearcountMINF$x[,2])
  
# From before
sigma.est <- 0.03226855
ar1.06.error <- (1 - 0.6^2)*sigma.est^2

ar_sim_data <- replicate(250, arima.sim(n = 109, model = list(ar = c(0.6)),
                                        sd = sqrt(ar1.06.error)))

# Would probably be faster with one function, but this is more readable

# Using corARMA with autoregressive order 1 and moving average order 0
# for AR(1), and vice-versa for MA1.  This way only one function needs to be 
# passed via clusterExport.

ar1_function <- function(x){
  z <- data.frame(Year = Data1$Year, x)
  p_value <- summary(gls(x ~ Year, data = z, 
                         correlation = corARMA(p=1, q=0)))$tTable[8]
  decision <- ifelse(p_value < 0.05, 1, 0)
  return(decision)
}

ma1_function <- function(x){
  z <- data.frame(Year = Data1$Year, x)
  p_value <- summary(gls(x ~ Year, data = z, 
                         correlation = corARMA(p = 0, q = 1)))$tTable[8]
  decision <- ifelse(p_value < 0.05, 1, 0)
  return(decision)
}

clusterExport(clust, c("Data1", "shuffle", "gls", "corARMA"))

t_ar_type_1 <- parApply(clust, ar_sim_data, 2, ar1_function)

t_ma_type_1 <- parApply(clust, ar_sim_data, 2, ma1_function)

# Since the seed is set for the cores and the regular PRNG this time, the 
# output won't change between runs. I checked.

save(t_ar_type_1, t_ma_type_1, file="p5.Rdata")
```

```{r p5a}
load("p5.Rdata")
type_1s <- cbind(mean(t_ar_type_1), mean(t_ma_type_1))
rownames(type_1s) <- "Type I Error"
colnames(type_1s) <- cbind("AR(1)", "MA(1)")

pander(type_1s)
```


## Some derivation practice (these can be handwritten). If you have not completed STAT 421 or equivalent, please try the problem and then take advantage of advanced help by stopping by to chat about your answer. 

6) Answer Cryer and Chan question 2.4 (page 20)  

For this problem, let $e_t \sim (0, \sigma^2)$ be a white noise process with mean 0 and variance $\sigma^2.$ Let $Y_t = e_t + \theta e_{t-1},$ where $\theta = 3 \text{or} \frac{1}{3}.$  

The variance of $Y_t$ (and $Y_{t-1}$) is 
\begin{align*}
\text{var}(Y_t) & = \text{var}(e_t) + \text{Var}(\theta e_{t-1}) \\
&= \sigma^2 + \theta^2 \sigma^2 \\
&= \sigma^2(1 + \theta^2)
\end{align*}

The covariance of $Y_t$ and $Y_{t-1}$ is  
\begin{align*}
\text{cov}(Y_t, Y_{t-1}) &= \text{cov}(e_t + \theta e_{t-1}, e_{t-1} + \theta e_{t-2}) \\
&= \text{cov}(e_t, e_{t-1}) + \text{cov}(e_t, \theta e_{t-2}) + \text{cov}(\theta e_{t-1}, e_{t-1}) + \text{cov}(\theta e_{t-1}, \theta e_{t-2}) \\
&= 0 + 0 + \theta \sigma^2 + 0 \\
& = \theta \sigma^2
\end{align*}

Because $e_t$ is a white noise process the covariance of $e_t$ and $e_{t-k}$ is 0 for all $k \neq 0.$  

Then, the autocorrelation function is:  
\[ \begin{cases} 
      1 & k = 0 \\
      \frac{\theta}{1+ \theta} & k = 1 \\
      0 & k > 1 
   \end{cases}
\]  
  


7) Suppose that we are interested in the properties of a local average (linear filter) of two observations from an original time series, $x_t$. The new series is $y_t=(0.5)*(x_{t-1}+x_t)$. The mean of $x_t$ is 3, the variance of $x_t$ is 4, and the correlation between any neighboring $x_t$'s is 0.5 (so $cor(x_t, x_{t-1})=0.5$). $x_t$'s more than two time points apart are uncorrelated (correlation is 0). Use the rules for means and variances of linear combinations to find $E(y_t)$, $Var(y_t)$, and $Cov(y_t,y_{t-1})$. Do not worry about what happens at the edges of the time series (for t=1 or t=n), only worry about $t$ in general. 

  - Note that you have some preliminary work to complete to go from the provided information to what you need to work on the three derivations requested.
